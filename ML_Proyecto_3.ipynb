{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "x_KcdXECGuxY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3oNtPFUl1_W",
        "outputId": "37d53bcc-8dbd-4709-cf87-122235d11f5e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/db.csv'\n",
        "db = pd.read_csv(file_path, header=None) # Read csv file with no header\n",
        "db.drop(db.tail(2).index,inplace = True) # Delete 2 last rows as they are totals"
      ],
      "metadata": {
        "id": "bnK0atZa_uLR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "\n",
        "X = db.iloc[:, 2:] # Get input data\n",
        "X = scaler.fit_transform(X) # Normalize data\n",
        "X = torch.from_numpy(X).float()\n",
        "\n",
        "Y = db.iloc[:, 1] # Get output\n",
        "Y = Y.replace({'M': 1, 'B': 0}) # 1 means malign, 0 means benign\n",
        "Y = np.array(Y)\n",
        "Y = torch.from_numpy(Y).float()\n",
        "\n",
        "#device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "cQ4VP1hvBi5s"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_layers, neurons_per_layer, output_size, activation_function):\n",
        "        super(MLP, self).__init__()\n",
        "        activation_functions = {\n",
        "            \"sigmoid\": nn.Sigmoid(),\n",
        "            \"tanh\": nn.Tanh(),\n",
        "            \"relu\": nn.ReLU()\n",
        "        }\n",
        "\n",
        "        if activation_function.lower() not in activation_functions:\n",
        "            raise ValueError(f\"Unsupported activation function: {activation_function}\")\n",
        "\n",
        "        self.activation_function = activation_functions[activation_function.lower()]\n",
        "\n",
        "        layers = []\n",
        "        # Input Layer\n",
        "        layers.append(nn.Linear(input_size, neurons_per_layer))\n",
        "        layers.append(self.activation_function)\n",
        "\n",
        "        # Hidden Layers\n",
        "        for i in range(hidden_layers):\n",
        "            layers.append(nn.Linear(neurons_per_layer, neurons_per_layer))\n",
        "            layers.append(self.activation_function)\n",
        "\n",
        "        # Output Layer\n",
        "        layers.append(nn.Linear(neurons_per_layer, output_size))\n",
        "\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "        self.loss_history = None\n",
        "\n",
        "    def forward(self, x):\n",
        "      output = self.layers(x)\n",
        "      return output.squeeze()\n",
        "\n",
        "    def fit(self, X_train, Y_train, epochs=3000, learning_rate=0.01):\n",
        "        loss_function = nn.BCEWithLogitsLoss() #Internally use Softmax function\n",
        "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "        loss_history = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "          Y_pred = self.forward(X_train)\n",
        "          loss = loss_function(Y_pred,Y_train)\n",
        "          loss_history.append(loss.item())\n",
        "          if (epoch%500==0):\n",
        "            print(f\"Epoch {epoch}: train loss: {loss.item()}\")\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "        self.loss_history = loss_history\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        self.eval()\n",
        "\n",
        "        with torch.no_grad():  # Deactive gradient calc for interference\n",
        "            Y_pred = self.forward(X_test)\n",
        "\n",
        "        probs = torch.sigmoid(Y_pred)\n",
        "\n",
        "        labels = probs > 0.5\n",
        "        return labels.int()  # Return 0 or 1\n"
      ],
      "metadata": {
        "id": "gz2WOHjoFE9Z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "G_V1GmlmhrxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuraciones para los experimentos\n",
        "hidden_layers_configs = [1, 2, 3]\n",
        "neurons_per_layer_configs = [16, 32, 64]\n",
        "activation_functions = [\"sigmoid\", \"tanh\", \"relu\"]\n",
        "\n",
        "# Diccionario para guardar los resultados\n",
        "results = {}\n",
        "\n",
        "for hidden_layers in hidden_layers_configs:\n",
        "    for neurons in neurons_per_layer_configs:\n",
        "        for activation_function in activation_functions:\n",
        "            print(f\"{hidden_layers} layers, {neurons} neurons, {activation_function}\")\n",
        "            # Crear y entrenar el modelo\n",
        "            model = MLP(input_size=X_train.shape[1], hidden_layers=hidden_layers, neurons_per_layer=neurons,\n",
        "                        output_size=1, activation_function=activation_function)\n",
        "            model.fit(X_train, Y_train)\n",
        "\n",
        "            # Evaluar el modelo\n",
        "            y_pred = model.predict(X_train)\n",
        "            accuracy = (y_pred == Y_train).float().mean()\n",
        "\n",
        "            print(\"------------------------------------------------\")\n",
        "            # Guardar los resultados\n",
        "            config_key = f\"{hidden_layers} layers, {neurons} neurons, {activation_function}\"\n",
        "            results[config_key] = {\n",
        "                \"loss_history\": model.loss_history,\n",
        "                \"accuracy\": accuracy.item()\n",
        "            }\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eA-Uh7mbyLHi",
        "outputId": "7e1eaee8-3479-4f12-c3af-1e49b28fda9b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 layers, 16 neurons, sigmoid\n",
            "Epoch 0: train loss: 0.6875467896461487\n",
            "Epoch 500: train loss: 0.022613557055592537\n",
            "Epoch 1000: train loss: 0.0031653502956032753\n",
            "Epoch 1500: train loss: 0.0009114059503190219\n",
            "Epoch 2000: train loss: 0.00037857875577174127\n",
            "Epoch 2500: train loss: 0.00019199847884010524\n",
            "------------------------------------------------\n",
            "1 layers, 16 neurons, tanh\n",
            "Epoch 0: train loss: 0.7128565907478333\n",
            "Epoch 500: train loss: 0.00857322383671999\n",
            "Epoch 1000: train loss: 0.0032973478082567453\n",
            "Epoch 1500: train loss: 0.0014465079875662923\n",
            "Epoch 2000: train loss: 0.00071050034603104\n",
            "Epoch 2500: train loss: 0.0003797796671278775\n",
            "------------------------------------------------\n",
            "1 layers, 16 neurons, relu\n",
            "Epoch 0: train loss: 0.6889375448226929\n",
            "Epoch 500: train loss: 0.08422309905290604\n",
            "Epoch 1000: train loss: 0.08062631636857986\n",
            "Epoch 1500: train loss: 0.07105127722024918\n",
            "Epoch 2000: train loss: 0.06772149354219437\n",
            "Epoch 2500: train loss: 0.06673706322908401\n",
            "------------------------------------------------\n",
            "1 layers, 32 neurons, sigmoid\n",
            "Epoch 0: train loss: 0.8344815373420715\n",
            "Epoch 500: train loss: 0.02275700494647026\n",
            "Epoch 1000: train loss: 0.003319096053019166\n",
            "Epoch 1500: train loss: 0.0008537736139260232\n",
            "Epoch 2000: train loss: 0.00031780527206137776\n",
            "Epoch 2500: train loss: 0.00015370591427199543\n",
            "------------------------------------------------\n",
            "1 layers, 32 neurons, tanh\n",
            "Epoch 0: train loss: 0.6753659844398499\n",
            "Epoch 500: train loss: 0.5393295884132385\n",
            "Epoch 1000: train loss: 0.003522428683936596\n",
            "Epoch 1500: train loss: 0.0012841332936659455\n",
            "Epoch 2000: train loss: 0.0005519083351828158\n",
            "Epoch 2500: train loss: 0.0002769546990748495\n",
            "------------------------------------------------\n",
            "1 layers, 32 neurons, relu\n",
            "Epoch 0: train loss: 0.6926364898681641\n",
            "Epoch 500: train loss: 0.0008418219513259828\n",
            "Epoch 1000: train loss: 0.00012202991638332605\n",
            "Epoch 1500: train loss: 4.417316449689679e-05\n",
            "Epoch 2000: train loss: 2.1729947548010387e-05\n",
            "Epoch 2500: train loss: 1.2380555745039601e-05\n",
            "------------------------------------------------\n",
            "1 layers, 64 neurons, sigmoid\n",
            "Epoch 0: train loss: 0.6618995666503906\n",
            "Epoch 500: train loss: 0.01775483600795269\n",
            "Epoch 1000: train loss: 0.003030502935871482\n",
            "Epoch 1500: train loss: 0.0008622209425084293\n",
            "Epoch 2000: train loss: 0.00033076657564379275\n",
            "Epoch 2500: train loss: 0.00015859314589761198\n",
            "------------------------------------------------\n",
            "1 layers, 64 neurons, tanh\n",
            "Epoch 0: train loss: 0.6852046847343445\n",
            "Epoch 500: train loss: 0.0031313521321862936\n",
            "Epoch 1000: train loss: 0.0002450509346090257\n",
            "Epoch 1500: train loss: 8.18740954855457e-05\n",
            "Epoch 2000: train loss: 3.9584025216754526e-05\n",
            "Epoch 2500: train loss: 2.248827695439104e-05\n",
            "------------------------------------------------\n",
            "1 layers, 64 neurons, relu\n",
            "Epoch 0: train loss: 0.7038002014160156\n",
            "Epoch 500: train loss: 0.0004571795288939029\n",
            "Epoch 1000: train loss: 7.663352880626917e-05\n",
            "Epoch 1500: train loss: 2.910958937718533e-05\n",
            "Epoch 2000: train loss: 1.4611079677706584e-05\n",
            "Epoch 2500: train loss: 8.373656783078332e-06\n",
            "------------------------------------------------\n",
            "2 layers, 16 neurons, sigmoid\n",
            "Epoch 0: train loss: 0.7001147866249084\n",
            "Epoch 500: train loss: 0.019609039649367332\n",
            "Epoch 1000: train loss: 0.0015898566925898194\n",
            "Epoch 1500: train loss: 0.00035888177808374166\n",
            "Epoch 2000: train loss: 0.0001440726191503927\n",
            "Epoch 2500: train loss: 7.406846998492256e-05\n",
            "------------------------------------------------\n",
            "2 layers, 16 neurons, tanh\n",
            "Epoch 0: train loss: 0.6701788306236267\n",
            "Epoch 500: train loss: 0.04888463020324707\n",
            "Epoch 1000: train loss: 0.005294167436659336\n",
            "Epoch 1500: train loss: 0.0009835315868258476\n",
            "Epoch 2000: train loss: 0.0002939838159363717\n",
            "Epoch 2500: train loss: 0.0001278198615182191\n",
            "------------------------------------------------\n",
            "2 layers, 16 neurons, relu\n",
            "Epoch 0: train loss: 0.7164419293403625\n",
            "Epoch 500: train loss: 0.014806265942752361\n",
            "Epoch 1000: train loss: 0.011635825037956238\n",
            "Epoch 1500: train loss: 0.0032366793602705\n",
            "Epoch 2000: train loss: 0.000876406382303685\n",
            "Epoch 2500: train loss: 0.00036318443017080426\n",
            "------------------------------------------------\n",
            "2 layers, 32 neurons, sigmoid\n",
            "Epoch 0: train loss: 0.6894692182540894\n",
            "Epoch 500: train loss: 0.017778931185603142\n",
            "Epoch 1000: train loss: 0.004288963973522186\n",
            "Epoch 1500: train loss: 0.0015301737003028393\n",
            "Epoch 2000: train loss: 0.0006023671594448388\n",
            "Epoch 2500: train loss: 0.00028081570053473115\n",
            "------------------------------------------------\n",
            "2 layers, 32 neurons, tanh\n",
            "Epoch 0: train loss: 0.7113688588142395\n",
            "Epoch 500: train loss: 0.027099724858999252\n",
            "Epoch 1000: train loss: 0.0004286579496692866\n",
            "Epoch 1500: train loss: 0.0001149334420915693\n",
            "Epoch 2000: train loss: 5.135099127073772e-05\n",
            "Epoch 2500: train loss: 2.7794552806881256e-05\n",
            "------------------------------------------------\n",
            "2 layers, 32 neurons, relu\n",
            "Epoch 0: train loss: 0.7035287618637085\n",
            "Epoch 500: train loss: 0.006299484521150589\n",
            "Epoch 1000: train loss: 0.0001505952823208645\n",
            "Epoch 1500: train loss: 3.2616433600196615e-05\n",
            "Epoch 2000: train loss: 1.4198513781593647e-05\n",
            "Epoch 2500: train loss: 7.800259481882676e-06\n",
            "------------------------------------------------\n",
            "2 layers, 64 neurons, sigmoid\n",
            "Epoch 0: train loss: 0.6807443499565125\n",
            "Epoch 500: train loss: 0.04575851187109947\n",
            "Epoch 1000: train loss: 0.014659247361123562\n",
            "Epoch 1500: train loss: 0.005498528480529785\n",
            "Epoch 2000: train loss: 0.047926027327775955\n",
            "Epoch 2500: train loss: 0.008176250383257866\n",
            "------------------------------------------------\n",
            "2 layers, 64 neurons, tanh\n",
            "Epoch 0: train loss: 0.6913323998451233\n",
            "Epoch 500: train loss: 0.0027286438271403313\n",
            "Epoch 1000: train loss: 0.00016437623708043247\n",
            "Epoch 1500: train loss: 5.623171819024719e-05\n",
            "Epoch 2000: train loss: 2.7865724405273795e-05\n",
            "Epoch 2500: train loss: 1.6200836398638785e-05\n",
            "------------------------------------------------\n",
            "2 layers, 64 neurons, relu\n",
            "Epoch 0: train loss: 0.6878369450569153\n",
            "Epoch 500: train loss: 0.00015150038234423846\n",
            "Epoch 1000: train loss: 2.1514235413633287e-05\n",
            "Epoch 1500: train loss: 8.020696441235486e-06\n",
            "Epoch 2000: train loss: 4.144075774092926e-06\n",
            "Epoch 2500: train loss: 2.4528267204004806e-06\n",
            "------------------------------------------------\n",
            "3 layers, 16 neurons, sigmoid\n",
            "Epoch 0: train loss: 0.6955185532569885\n",
            "Epoch 500: train loss: 0.04767543449997902\n",
            "Epoch 1000: train loss: 0.028375742956995964\n",
            "Epoch 1500: train loss: 0.019139738753437996\n",
            "Epoch 2000: train loss: 0.01727200113236904\n",
            "Epoch 2500: train loss: 0.01317296177148819\n",
            "------------------------------------------------\n",
            "3 layers, 16 neurons, tanh\n",
            "Epoch 0: train loss: 0.7120137214660645\n",
            "Epoch 500: train loss: 0.022070249542593956\n",
            "Epoch 1000: train loss: 0.013107169419527054\n",
            "Epoch 1500: train loss: 0.016620393842458725\n",
            "Epoch 2000: train loss: 0.004203421995043755\n",
            "Epoch 2500: train loss: 0.00012253473687451333\n",
            "------------------------------------------------\n",
            "3 layers, 16 neurons, relu\n",
            "Epoch 0: train loss: 0.7251580953598022\n",
            "Epoch 500: train loss: 0.017588291317224503\n",
            "Epoch 1000: train loss: 0.00947598833590746\n",
            "Epoch 1500: train loss: 0.0021832494530826807\n",
            "Epoch 2000: train loss: 0.0014006110141053796\n",
            "Epoch 2500: train loss: 0.02828785590827465\n",
            "------------------------------------------------\n",
            "3 layers, 32 neurons, sigmoid\n",
            "Epoch 0: train loss: 0.6828107833862305\n",
            "Epoch 500: train loss: 0.05231552943587303\n",
            "Epoch 1000: train loss: 0.04739784821867943\n",
            "Epoch 1500: train loss: 0.020756836980581284\n",
            "Epoch 2000: train loss: 0.017765896394848824\n",
            "Epoch 2500: train loss: 0.017007950693368912\n",
            "------------------------------------------------\n",
            "3 layers, 32 neurons, tanh\n",
            "Epoch 0: train loss: 0.6699724793434143\n",
            "Epoch 500: train loss: 0.02695811539888382\n",
            "Epoch 1000: train loss: 0.00021250547433737665\n",
            "Epoch 1500: train loss: 4.3101361370645463e-05\n",
            "Epoch 2000: train loss: 1.9121100194752216e-05\n",
            "Epoch 2500: train loss: 1.0660338375600986e-05\n",
            "------------------------------------------------\n",
            "3 layers, 32 neurons, relu\n",
            "Epoch 0: train loss: 0.6932750940322876\n",
            "Epoch 500: train loss: 7.324133912334219e-05\n",
            "Epoch 1000: train loss: 1.3035560186835937e-05\n",
            "Epoch 1500: train loss: 5.105351192469243e-06\n",
            "Epoch 2000: train loss: 2.5906322207447374e-06\n",
            "Epoch 2500: train loss: 1.5035203659863328e-06\n",
            "------------------------------------------------\n",
            "3 layers, 64 neurons, sigmoid\n",
            "Epoch 0: train loss: 0.6995629668235779\n",
            "Epoch 500: train loss: 0.01696278713643551\n",
            "Epoch 1000: train loss: 0.0016298253322020173\n",
            "Epoch 1500: train loss: 0.0002854351478163153\n",
            "Epoch 2000: train loss: 0.00010390171519247815\n",
            "Epoch 2500: train loss: 5.0539001676952466e-05\n",
            "------------------------------------------------\n",
            "3 layers, 64 neurons, tanh\n",
            "Epoch 0: train loss: 0.6963040828704834\n",
            "Epoch 500: train loss: 0.0389111265540123\n",
            "Epoch 1000: train loss: 0.00016339801368303597\n",
            "Epoch 1500: train loss: 5.30480028828606e-05\n",
            "Epoch 2000: train loss: 2.656113065313548e-05\n",
            "Epoch 2500: train loss: 1.5470448488486e-05\n",
            "------------------------------------------------\n",
            "3 layers, 64 neurons, relu\n",
            "Epoch 0: train loss: 0.6907364130020142\n",
            "Epoch 500: train loss: 0.00011030775203835219\n",
            "Epoch 1000: train loss: 1.5992101907613687e-05\n",
            "Epoch 1500: train loss: 6.369546099449508e-06\n",
            "Epoch 2000: train loss: 3.3234236980206333e-06\n",
            "Epoch 2500: train loss: 1.99164833247778e-06\n",
            "------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_dic = {'sigmoid': [], 'relu': [], 'tanh': []}  # Actualiza con tus datos\n",
        "\n",
        "for config, result in results.items():\n",
        "    # Extraer la función de activación del nombre de la configuración\n",
        "    activation_function = config.split(', ')[-1]\n",
        "    accuracy_dic[activation_function].append(result['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "#print(accuracy_dic)\n",
        "\n",
        "for config, result in results.items():\n",
        "    # Extraer la función de activación del nombre de la configuración\n",
        "    activation_function = config.split(', ')[-1]\n",
        "    if (result['accuracy'] == max(accuracy_dic[activation_function]) and activation_function == 'relu'):\n",
        "      print(config)\n",
        "      print('----------------------')\n",
        "      # plt.plot(result['loss_history'], label=activation_function)\n",
        "      # plt.title('Función de Pérdida vs Épocas por Función de Activación')\n",
        "      # plt.xlabel('Épocas')\n",
        "      # plt.ylabel('Pérdida')\n",
        "      # plt.legend()\n",
        "      # plt.show()\n",
        "\n",
        "# # Crear un gráfico para comparar las funciones de activación\n",
        "# #plt.figure(figsize=(10, 6))\n",
        "# accuracy_dic = {'sigmoid': [], 'relu': [], 'tanh': []}\n",
        "\n",
        "\n",
        "# # Crear el gráfico de líneas\n",
        "# plt.figure(figsize=(10, 6))\n",
        "\n",
        "# # Iterar sobre cada función de activación y graficar sus precisiones\n",
        "# for activation_function, accuracies in accuracy_dic.items():\n",
        "#     plt.plot(accuracies, label=activation_function)\n",
        "\n",
        "# plt.title('Valores de Precisión para Cada Función de Activación')\n",
        "# plt.xlabel('Experimento / Configuración')\n",
        "# plt.ylabel('Precisión')\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4yKGpz9h2Xz",
        "outputId": "08bbf443-9f2e-4e68-cf75-5b6f06bcca5a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 layers, 32 neurons, relu\n",
            "----------------------\n",
            "1 layers, 64 neurons, relu\n",
            "----------------------\n",
            "2 layers, 16 neurons, relu\n",
            "----------------------\n",
            "2 layers, 32 neurons, relu\n",
            "----------------------\n",
            "2 layers, 64 neurons, relu\n",
            "----------------------\n",
            "3 layers, 32 neurons, relu\n",
            "----------------------\n",
            "3 layers, 64 neurons, relu\n",
            "----------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Flt4fIAO0PRh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}